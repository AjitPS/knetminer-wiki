# Deploying Knetminer with Docker


## Introduction

Knetminer can be deployed in a quick and simple way, using the Docker container platform. In 
the simplest case, you just need to install the Docker environment and get our
[Knetminer image from DockerHub][10]. For more advanced uses, our image can be used with 
host-provided configurations. Additionally, you can customise our [image definitions](TODO). 
Details are explained in the follow.  

[10]: https://cloud.docker.com/u/knetminer/repository/docker/knetminer/knetminer


## Running Knetminer via Docker

### Quick start with the sample dataset

An instance of Knetminer running against a small sample data set (a subset of the 
[Arabidopsis Thaliana](https://knetminer.rothamsted.ac.uk/Arabidopsis_thaliana/), just 
install Docker and issue this line commad:

`docker run -p 8080:8080 -it knetminer/knetminer`

This will get our Knetminer image straight from Docker Hub. You'll see output on your 
shell Window and eventually you'll be able to access the sample application via 
http://localhost:8080/client (as usually, you can change the port on the host via `-p`).  

Running without further parameters makes our container to use the configuration and data it 
has in its file system. By using volume-related parameters, you can reuse the same image 
(without rebuilds) to instantiated Knetminer against configuration and data files located 
on your host (more in [this section](TODO)).  

*Note: the `-it` options are needed because of the startup command that our container runs, 
as explained [here](https://stackoverflow.com/a/41099052/529286).*


### Building and running the Knetminer image 

Optionally, you can build our image from our source files and run it as above. To do so, 
git-clone our [code base](https://github.com/Rothamsted/knetminer) and issue this command 
**from the root of the cloned repository**:

`docker build -t knetminer/knetminer -f common/quickstart/Dockerfile .`

Note that the `-t` is optional and, if you use it, it would be advisable to use your own tag 
for the newly built image (eg, `a_dockhub_user/knetminer`).  


### Instantiating a specific dataset 

The built image can be run agains one of the existing pre-packaged configurations, each 
representing knowledge bases (ie, datasets) for different species (Usually, one Knetminer 
datasets contains information focused on a given specie). This makes it possible to re-use 
one image to run multiple Knetminer applications, each dedicated to a particular specie.

Predefined configurations are stored in our codebase, under the [specie/][90] directory.

Suppose you want to run a Knetminer instance for the [arabidopsis][200] dataset. 
You can do that as follows.

  1. Create a dataset directory on your running host, let's say 
  `/home/knetminer/arabidopsis`
    1. Under that directory, create the `config/`  and `data` subdirectories
    2. Copy the Arabidopsis OXL file into `/home/knetminer/arabidopsis/data/knowledge-base.oxl`. By default, all the Ketminer configurations look for this name into the data directory to get the dataset file.
  2. Run the command:
```bash
docker run -it -p 8080:8080 \
       --volume /home/knetminer/arabidopsis/data:/root/knetminer-data \
       --volume /home/knetminer/arabidopsis/config:/root/knetminer-config \
       knetminer/knetminer
```

The startup phase of large datasets takes long time. After that, the application will be 
available at `http://localhost:8080/client`, as in the previous section. Similarly, it can 
be stopped via `Control-C`.  


### What's happening? 

The same image used for the sample dataset is used, however, this time the container
doesn't see the default files in its file system, under `/root/knetminer-data` and 
`/root/knetminer-config`. Rather, those directories are mapped to host directories, via the 
[Docker volumes mechanism][210], and therefore the image instance will use those
(dataset-specific) files. This mechanism is widely used by our Docker architecture to 
customise the base image beyond the sample data it uses by default.

[200]: /Rothamsted/knetminer/tree/201904_new_docker/species
[210]: https://stephenafamo.com/blog/docker-volumes-introduction/

### Running with `docker-run.sh`

The crafting of the above `docker run` command line can be easied by means of the
[`docker-run.sh`][220] script. This receives the specie id (usually a directory path 
relative to our [specie directory][200]) and environment variables about parameters like the 
config or the data directory. It then builds the docker command line 
with the right options.  

Likely, you will want to write your own script to invoke `docker-run.sh` against your 
host directories and your particular dataset. [This other script][230], which we use to test 
Knetminer on our servers, is an example (it contains Neo4j-related parameters, which we 
describe in the follow, you can safely ignore them until you want to use Neo4j).

[220]: /Rothamsted/knetminer/tree/201904_new_docker/common/quickstart/docker-run.sh
[230]: /Rothamsted/knetminer/tree/201904_new_docker/common/quickstart/docker-run-rres-test.sh

### Enabling Neo4j

TODO

## Configuring a Knetminer container

You you have already opened files under the [species][90] folder, you got an ide of how 
Knetminer configuration and build files looks like. In this section, we go through the 
details. Note that the notation `<specie>` we use here refers to one of the directories 
under the mentioned specie folder (`arabidopsis`, `wheat`, `rice`, etc). Remember that each 
of those directories contains the files needed to instantiate a container against a given 
dataset (which usually is focused on a single specie).

To summarise the details given below. For each species you have to take care of the following files:

  * `<specie>/maven-settings.xml`, defining a number of configuration values
  * `<specie>/client/*`, defining specie-specific web files for the client application
  * `<specie>/ws/SemanticMotifs.txt`, defining graph patterns that relate genes to other entities of interest.
  * `<specie>/ws/neo4j/` to be used in Neo4j mode (TODO: details).

### Maven settings

This are mostly defined by `<specie>/maven-settings.xml` The values in this file are used to 
instantiate Maven build variables throughout the [aratiny][60] project, which is a 
test/reference implementation of a Knetminer application, composed by two WAR web 
applications: `aratiny-ws`, the Knetminer web service (often called 'the server'), providing 
raw/programmatic access to Knetminer functionality (often called 'the server') and the WAR 
`aratiny-client`, the front end web application that shows the Knetminer web interface.

The variables in the specie-specific Maven settings override defaults defined in the 
[Knetminer's main POM][240] (both in the `<properties>` and `<profiles>` sections).

[240]: /Rothamsted/knetminer/blob/201904_new_docker/pom.xml

### Client

These are located in `<species>/client` and contains:
  * instance-specific web files for the client WAR, eg, `organism.png`, `background.png`, 
  `release_notes.html`
  * instance-specific configuration files put into the client WAR. At the moment the only 
  file of this type is `sampleQuery.xml` which defines sample queries appearing on the right 
  column of the Knetminer interface.

Client files are used by the Docker container **at runtime**, after the corresponding image 
has been built. In fact, the Docker container uses the files in 
`<species>/client` to override the reference client application and then rebuild the 
corresponding WAR, before passing it to the Tomcat server and launching it.

### Server files

Part of the Maven settings, either specie-specific or defaults are used to instantiate a 
server instance's configuration files. the main one of these is [data-source.xml][250], 
which is the first config item the -ws application looks for during its bootstrap stage. 
In turn, this file points to other paths, like `${knetminer.dataDir}`, the location where 
application-generated data are stored (eg, Lucene Indexes), or `${knetminer.oxlFile}`, the 
location where the dataset's OXL file is.  

Note that these two locations aren't specie-specific in Docker: the data dir is always 
`/root/knetminer-data` and the OXL file is always
`/root/knetminer-data/knowledge-network.oxl`. That's because, as explained above, the final 
location of these directories and files is to be established by means of volume-based 
mapping. That's why you don't find the corresponding variables in 
`<specie>/maven-settings.xml`.Rather, they're defined in the [main POM][240], (the `docker` 
Maven profile defines the Docker-specific values).

Other files in `<species>/ws/` are: 
  * `SemanticMotifs.txt`, which, as said above, defines graph pattern to relate genes to 
  other entities of interest. This is used for [ranking gene searches][260] based on gene evidence.
  * `neo4j/`, which contains configuration related to Knetminer running in Neo4j mode 
  (TODO: additions due).

Note that, differently than the client, the server WAR is generated once for all **at image 
build time**, not at runtime. That's because the WAR is independent on any specific dataset 
you want run Knetminer with. However, the runtime still uses Maven to instantiate (in Maven 
terms, interpolate) client and server configuration files with specie/dataset specific 
values.

[250]: /Rothamsted/knetminer/blob/201904_new_docker/common/aratiny/aratiny-ws/src/test/resources/knetminer-config/data-source.xml
[260]: https://pub.uni-bielefeld.de/publication/2915227


## The Knetminer Docker architecture

### Docker images
As usually, the main contained is defined by the [Dockerfile image file][20]. This builds 
what is needed to deploy our web applications (two .war files) on our Java web server 
(Tomcat).

This application image extends the [Dockerfile-base image][30]. This prepares a container 
with all the Maven-reachable dependencies downloaded and put in place into the container file 
system.

Finally, the base image depends on [Dockerfile-bare image][40]. This pulls up the 
[Andre's Tomcat container][50], which is eseentially a Debian with Java and Tomcat installed, 
and additionally deploys third-party dependencies that are needed by Knetminer (eg, Maven, 
the Node.js package manager, NPM).

The motivation for distributing the build of the final container into three different images 
is that this makes the development quicker. For instance, if one need just to change a web 
application page, rebuilding the main container only achieve that the changes are moved 
forward, without having to rebuild any other dependencies.


### Dataset-independent container

The Dockerfile container performs several application building operations at runtime, once 
the image has been created. Namely, 
  * It places configuration files on a fixed directory on the container filesystem.
  * It builds a reference [server application][60] (ie, the Knetminer web service API) and 
  places it into the Tomcat installation on the container.
  * It prepares links for the [client application][60] (ie, front-end).
  * Creates some optional settings (eg, makes the Tomcat manager application accessible).
  
The Docker image building script **doesn't** build the client WAR at the Docker image 
building time, since the exact contents for this depend on the specific dataset that you want 
serve with Knetminer.

This is the job of the [runtime helper][70]. This is set as [container entry point][80] and 
uses Maven to build the Knetminer client at run time, configuring it with a user-defined 
configuration (which is usually put in some host directory and passed to the container as 
mapped volume).

That file does the following:
  * It takes dataset (ie, specie)-specific files (the instance files) and uses them to 
  prepare a server configuration (eg, to populate `data-sources.xml`, to provide 
  `SemanticMotifs.txt`), by etiher making copies to a config directory, or instantiating 
  template files using Maven settings (which are defined in the instance directory as well). 
  These instance files are usually stored in some directory under the
  [species/ directory][90], under the Knetminer codebase.
  * Similarly, it takes dataset-specific client files, places them on a build directory and 
  uses them to build a client that is specific for the dataset being run.
  * Runs the Tomcat server in the container, which sees the created configuration and the 
  built client WAR. 

Configuration files and the build location can be mapped to an host location, as usually, via Docker volumes.
This is done by the [docker-run.sh script][100]. (TODO: review)
  

[20]: /Rothamsted/knetminer/blob/201904_new_docker/common/quickstart/Dockerfile
[30]: /Rothamsted/knetminer/blob/201904_new_docker/common/quickstart/Dockerfile-base
[40]: /Rothamsted/knetminer/blob/201904_new_docker/common/quickstart/Dockerfile-bare
[50]: https://hub.docker.com/r/andreptb/tomcat
[60]: /Rothamsted/knetminer/tree/master/common/aratiny
[70]: /Rothamsted/knetminer/blob/201904_new_docker/common/quickstart/runtime-helper.sh
[80]: https://aws.amazon.com/blogs/opensource/demystifying-entrypoint-cmd-docker
[90]: /Rothamsted/knetminer/tree/201904_new_docker/species
[100]: /Rothamsted/knetminer/blob/201904_new_docker/common/quickstart/docker-run.sh
